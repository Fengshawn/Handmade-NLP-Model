{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM4513-6513] Assignment 2: Text Classification with a Feedforward Network\n",
    "\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop a Feedforward network for text classification. \n",
    "\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "- Text processing methods for transforming raw text data into input vectors for your network  (**1 mark**)\n",
    "- A Feedforward network consisting of:\n",
    "    - **One-hot** input layer mapping words into an **Embedding weight matrix** (**1 mark**)\n",
    "    - **One hidden layer** computing the mean embedding vector of all words in input followed by a **ReLU activation function** (**1 mark**)\n",
    "    - **Output layer** with a **softmax** activation. (**1 mark**)\n",
    "- The Stochastic Gradient Descent (SGD) algorithm with **back-propagation** to learn the weights of your Neural network. Your algorithm should:\n",
    "    - Use (and minimise) the **Categorical Cross-entropy loss** function (**1 mark**)\n",
    "    - Perform a **Forward pass** to compute intermediate outputs (**4 marks**)\n",
    "    - Perform a **Backward pass** to compute gradients and update all sets of weights (**4 marks**)\n",
    "    - Implement and use **Dropout** after each hidden layer for regularisation (**2 marks**)\n",
    "- Discuss how did you choose hyperparameters? You can tune the learning rate (hint: choose small values), embedding size {e.g. 50, 300, 500}, the dropout rate {e.g. 0.2, 0.5} and the learning rate. Please use tables or graphs to show training and validation performance for each hyperparam combination  (**2 marks**). \n",
    "- After training the model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot and report accuracy.\n",
    "- Re-train your network by using pre-trained embeddings ([GloVe](https://nlp.stanford.edu/projects/glove/)) trained on large corpora. Instead of randomly initialising the embedding weights matrix, you should initialise it with the pre-trained weights. During training, you should not update them (i.e. weight freezing) and backprop should stop before computing gradients for updating embedding weights. Report results by performing hyperparameter tuning and plotting the learning process. Do you get better performance? (**3 marks**).\n",
    "\n",
    "- **BONUS:** Extend you Feedforward network by adding more hidden layers (e.g. one more). How does it affect the performance? Note: You need to repeat hyperparameter tuning, but the number of combinations grows exponentially. Therefore, you need to choose a subset of all possible combinations (**+2 extra marks**)\n",
    "\n",
    "\n",
    "\n",
    "### Data \n",
    "\n",
    "The data you will use for Task 2 is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "### Pre-trained Embeddings\n",
    "\n",
    "You can download pre-trained GloVe embeddings trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) from [here](http://nlp.stanford.edu/data/glove.840B.300d.zip). No need to unzip, the file is large.\n",
    "\n",
    "### Save Memory\n",
    "\n",
    "To save RAM, when you finish each experiment you can delete the weights of your network using `del W` followed by Python's garbage collector `gc.collect()`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment2.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex`).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc.. You are allowed to re-use your code from Assignment 1.\n",
    "\n",
    "Please make sure to comment your code. You should also mention if you've used Windows to write and test your code. There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1 of ~75-80% and ~85% without and with using pre-trained embeddings respectively. \n",
    "\n",
    "This assignment will be marked out of 20. It is worth 20\\% of your final grade in the module. If you implement the bonus question you can get up to 2 extra points but your final grade will be capped at 20.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Mon, 18 May 2020** and it needs to be submitted via Blackboard (MOLE). Standard departmental penalties for lateness will be applied. We use a range of strategies to detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index), including Turnitin which helps detect plagiarism, so make sure you do not plagiarise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:00:18.625532Z",
     "start_time": "2020-04-02T15:00:17.377733Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from time import localtime, strftime\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "import zipfile\n",
    "import gc\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "# This script was written on Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw texts into training and development data\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.748484Z",
     "start_time": "2020-04-02T14:26:39.727404Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"./data_topic/train.csv\",header=None) \n",
    "data_test = pd.read_csv(\"./data_topic/test.csv\",header=None)\n",
    "data_dev = pd.read_csv(\"./data_topic/dev.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.columns=[\"label\",\"text\"]\n",
    "data_test.columns=[\"label\",\"text\"]\n",
    "data_dev.columns=[\"label\",\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.753874Z",
     "start_time": "2020-04-02T14:26:39.749647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Venezuelans turned out early\\and in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - South Korean police used water canno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Thousands of Palestinian\\prisoners i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Sporadic gunfire and shelling took place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - Dozens of Rwandan soldiers flew into Suda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Reuters - Venezuelans turned out early\\and in ...\n",
       "1      1  Reuters - South Korean police used water canno...\n",
       "2      1  Reuters - Thousands of Palestinian\\prisoners i...\n",
       "3      1  AFP - Sporadic gunfire and shelling took place...\n",
       "4      1  AP - Dozens of Rwandan soldiers flew into Suda..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the raw texts into python array\n",
    "\n",
    "X_tr_raw = data_train.iloc[:,1]\n",
    "X_te_raw  = data_test.iloc[:,1]\n",
    "X_dev_raw = data_dev.iloc[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input representations\n",
    "\n",
    "\n",
    "To train your Feedforward network, you first need to obtain input representations given a vocabulary. One-hot encoding requires large memory capacity. Therefore, we will instead represent documents as lists of vocabulary indices (each word corresponds to a vocabulary index). \n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of words. You should: \n",
    "- tokenise all texts into a list of unigrams (tip: you can re-use the functions from Assignment 1) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- remove unigrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of the top-N most frequent unigrams in the entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:40.851926Z",
     "start_time": "2020-04-02T14:26:40.847500Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i', 'if',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
    "              'but', 'not', 'there', 'no', 'does', 'not', 'so', 've', 'their',\n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:41.505459Z",
     "start_time": "2020-04-02T14:26:41.498388Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', stop_words=[], vocab=set()):\n",
    "    \n",
    "    unigram = re.compile(token_pattern,re.I).findall(x_raw)#tokenise\n",
    "    # convert words to lowercase\n",
    "    for i in range(len(unigram)):\n",
    "        unigram[i] = unigram[i].lower()   \n",
    "    word_list = unigram[:] #make a copy of the word_list\n",
    "    \n",
    "    #remove stop words\n",
    "    for word in unigram:   \n",
    "        if word in stop_words:\n",
    "            word_list.remove(word)\n",
    "    \n",
    "    #create ngram lists according to ngram_range\n",
    "    ngram_range_list = list(range(ngram_range[0],ngram_range[1]+1))  \n",
    "    x_temp=[]\n",
    "    for j in ngram_range_list:\n",
    "        if j==1:\n",
    "            for i in range(0,len(word_list)):\n",
    "                x_temp.append(word_list[i])       \n",
    "        if j>1:\n",
    "            for i in range(0,len(word_list)):\n",
    "                if i+j <= len(word_list):\n",
    "                    x_temp.append(tuple(word_list[i:i+j]))\n",
    "                \n",
    "    #output final result x according to vocab\n",
    "    x=[]\n",
    "    if vocab == set():#by default, vocab==set(),in this case, word list don't need to be processed\n",
    "        x=x_temp\n",
    "    else:\n",
    "        vocab = list(vocab)#if given a list of vocabulary, function should return the words needed\n",
    "        for element in x_temp:\n",
    "            if element in vocab:\n",
    "                x.append(element)\n",
    "\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:42.563876Z",
     "start_time": "2020-04-02T14:26:42.557967Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "              min_df=0, keep_topN=0, stop_words=[]):\n",
    "    \n",
    "    #create lists for counting document frequency and collection frequency\n",
    "    lists_df=[] \n",
    "    lists_cf=[]\n",
    "    for x_raw in X_raw:\n",
    "        temp_cf = extract_ngrams(x_raw, ngram_range,token_pattern,stop_words)\n",
    "        temp_df = list(set(temp_cf)) #remove repeat element in each document for counting document frequency\n",
    "        lists_cf += temp_cf\n",
    "        lists_df += temp_df\n",
    "    \n",
    "    #count document frequency\n",
    "    df = Counter()\n",
    "    for ngram in lists_df:\n",
    "        df[ngram] += 1\n",
    "    if min_df != 0:\n",
    "        df = {k:v for k,v in df.items() if v>=min_df}\n",
    "        df = sorted(df.items(),key=lambda x:x[1],reverse=True)\n",
    "        df = Counter(dict(df))\n",
    "    \n",
    "    #count collection frequency    \n",
    "    cf = Counter()\n",
    "    for ngram in lists_cf:\n",
    "        cf[ngram] += 1\n",
    "    ngram_counts = cf #ngram_counts is the collection frequency\n",
    "    \n",
    "    #get vocab according to keep_topN\n",
    "    if keep_topN != 0:\n",
    "        tmp = {k:v for k,v in ngram_counts.items()}\n",
    "        tmp = sorted(tmp,key=lambda x:tmp[x],reverse=True)\n",
    "        vocab = tmp[:keep_topN]\n",
    "        vocab = set(vocab)\n",
    "    else:\n",
    "        vocab = set(lists_df)\n",
    "    \n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:43.577997Z",
     "start_time": "2020-04-02T14:26:43.478950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8931\n",
      "\n",
      "['declared', 'universal', 'uae', 'quit', 'improbable', 'pledge', 'kreme', 'climax', 'ankle', 'farce', 'utterly', 'jumped', 'lorry', 'commander', 'eu', 'amat', 'edge', 'withholding', 'writer', 'seasonally', 'criticized', 'damion', 'mon', 'operative', 'bengals', 'herself', 'winner', 'should', 'clergy', 'jarno', 'cremated', 'kostas', 'exhale', 'coroibos', 'benefits', 'dispatched', 'grip', 'al', 'fullback', 'athletes', 'clear', 'ipod', 'deliver', 'grave', 'platform', 'tohmatsu', 'michigan', 'remain', 'immortality', 'approved', 'shortage', 'ten', 'prepared', 'survives', 'outlook', 'fellows', 'rolls', 'mot', 'actual', 'kill', 'shipments', 'drafty', 'evening', 'freddy', 'amanda', 'attacked', 'evident', 'tail', 'surprise', 'deeply', 'subcontractors', 'learned', 'much', 'sitting', 'heinz', 'recordings', 'developers', 'body', 'coached', 'wellington', 'formally', 'also', 'saskatchewan', 'electric', 'travis', 'ingredient', 'withdrawn', 'yet', 'beverages', 'sense', 'station', 'andruw', 'gadgets', 'pipeline', 'revenue', 'weaken', 'outstrip', 'constant', 'others', 'patch']\n",
      "\n",
      "[('reuters', 631), ('said', 432), ('tuesday', 413), ('wednesday', 344), ('new', 325), ('after', 295), ('ap', 275), ('athens', 245), ('monday', 221), ('first', 210)]\n"
     ]
    }
   ],
   "source": [
    "vocab, df, ngram_counts = get_vocab(X_tr_raw, ngram_range=(1,1),keep_topN=10000,stop_words=stop_words)\n",
    "\n",
    "print(len(vocab))\n",
    "print()\n",
    "print(list(vocab)[:100])\n",
    "print()\n",
    "print(df.most_common()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and id -> word dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = dict(zip(vocab,range(len(vocab))))\n",
    "id2word = dict(zip(range(len(vocab)),vocab))\n",
    "\n",
    "# set a unkown token, use the last index number as 'unknown'\n",
    "word2id[str(len(word2id))] = len(word2id)\n",
    "id2word[len(id2word)] = str(len(id2word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix. \n",
    "\n",
    "First, represent documents in train, dev and test sets as lists of words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert texts into ngrams\n",
    "X_uni_tr = []\n",
    "for oneDoc in X_tr_raw:\n",
    "    X_uni_tr += [extract_ngrams(oneDoc,ngram_range=(1,1), stop_words=stop_words)]\n",
    "\n",
    "X_uni_dev = []\n",
    "for oneDoc in X_dev_raw:\n",
    "    X_uni_dev += [extract_ngrams(oneDoc,ngram_range=(1,1), stop_words=stop_words)]\n",
    "\n",
    "X_uni_te = []\n",
    "for oneDoc in X_te_raw:\n",
    "    X_uni_te += [extract_ngrams(oneDoc,ngram_range=(1,1), stop_words=stop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuters',\n",
       " 'venezuelans',\n",
       " 'turned',\n",
       " 'out',\n",
       " 'early',\n",
       " 'large',\n",
       " 'numbers',\n",
       " 'sunday',\n",
       " 'vote',\n",
       " 'historic',\n",
       " 'referendum',\n",
       " 'either',\n",
       " 'remove',\n",
       " 'left',\n",
       " 'wing',\n",
       " 'president',\n",
       " 'hugo',\n",
       " 'chavez',\n",
       " 'office',\n",
       " 'give',\n",
       " 'him',\n",
       " 'new',\n",
       " 'mandate',\n",
       " 'govern',\n",
       " 'next',\n",
       " 'two',\n",
       " 'years']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_uni_tr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert them into lists of indices in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to convert input document(each document represented by unigrams) into a list of vocabulary indices\n",
    "def ngram2vocabIndices(X_uni,ngram_range=(1,1),vocab=set()):\n",
    "    \n",
    "    indices=[] # store vocabulary indices\n",
    "    l = len(vocab)-1\n",
    "    for oneDoc in X_uni:\n",
    "        temp = []\n",
    "        for word in oneDoc:\n",
    "            if word in vocab: #vocab in this case is word2id\n",
    "                temp += [vocab[word]]\n",
    "            else: # if the word in test set or dev set are not exist in vocab, set this word to a \"unknown\" token  \n",
    "                temp += [l]       \n",
    "        indices += [temp]\n",
    "        \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.752658Z",
     "start_time": "2020-04-02T14:26:45.730409Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr = ngram2vocabIndices(X_uni_tr,vocab=word2id)\n",
    "X_dev = ngram2vocabIndices(X_uni_dev,vocab=word2id)\n",
    "X_te = ngram2vocabIndices(X_uni_te,vocab=word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5740,\n",
       " 3644,\n",
       " 1011,\n",
       " 6431,\n",
       " 4557,\n",
       " 1211,\n",
       " 1230,\n",
       " 7936,\n",
       " 8862,\n",
       " 2354,\n",
       " 6418,\n",
       " 3951,\n",
       " 7235,\n",
       " 4258,\n",
       " 1129,\n",
       " 8914,\n",
       " 950,\n",
       " 3683,\n",
       " 532,\n",
       " 8024,\n",
       " 238,\n",
       " 1889,\n",
       " 210,\n",
       " 8533,\n",
       " 1237,\n",
       " 3037,\n",
       " 7239]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3700,\n",
       " 3427,\n",
       " 8790,\n",
       " 3406,\n",
       " 8701,\n",
       " 4853,\n",
       " 3966,\n",
       " 3037,\n",
       " 4962,\n",
       " 989,\n",
       " 7166,\n",
       " 1914,\n",
       " 122,\n",
       " 6994,\n",
       " 2247,\n",
       " 6731,\n",
       " 8200,\n",
       " 1821,\n",
       " 1490,\n",
       " 5884,\n",
       " 4806,\n",
       " 6444,\n",
       " 8931,\n",
       " 1039,\n",
       " 3738,\n",
       " 5784,\n",
       " 4883,\n",
       " 8931]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev[0] # in this case 8931 represent all the word not exsited in training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the labels `Y` for train, dev and test sets into arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:03:13.183996Z",
     "start_time": "2020-04-02T15:03:13.077575Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_tr = np.array(data_train.iloc[:,0])\n",
    "Y_te = np.array(data_test.iloc[:,0])\n",
    "Y_dev = np.array(data_dev.iloc[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "\n",
    "Your network should pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W^T) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "During training, $\\mathbf{a}_1$ should be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "You can extend to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i^T $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n",
    "\n",
    "\n",
    "# Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initiliasing the weight matrices. For that purpose, you should implement the `network_weights` function that takes as input:\n",
    "\n",
    "- `vocab_size`: the size of the vocabulary\n",
    "- `embedding_dim`: the size of the word embeddings\n",
    "- `hidden_dim`: a list of the sizes of any subsequent hidden layers (for the Bonus). Empty if there are no hidden layers between the average embedding and the output layer \n",
    "- `num_clusses`: the number of the classes for the output layer\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: a dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers (hint: use numpy.random.uniform with from -0.1 to 0.1)\n",
    "\n",
    "See the examples below for expected outputs. Make sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise you won't be able to perform forward and backward passes. Consider also using np.float32 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:10.086665Z",
     "start_time": "2020-04-02T15:09:10.083429Z"
    }
   },
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=1000, embedding_dim=300, \n",
    "                    hidden_dim=[], num_classes=3, init_val = 0.5):\n",
    "    \n",
    "    W = {}\n",
    "    W_emb = np.random.uniform(-0.1,0.1,(vocab_size,embedding_dim))\n",
    "    W[0] = W_emb # W_emb must be the 0th in the dictionary\n",
    "    index = 1 # use a flag to mark current index\n",
    "    last_len = W[0].shape[1] # use a varible to store length number of last layer\n",
    "    \n",
    "    if hidden_dim!=[]:\n",
    "        for layer in hidden_dim:\n",
    "            W_hi = np.random.uniform(-0.1,0.1,(last_len,layer))\n",
    "            W[index] = W_hi\n",
    "            index += 1\n",
    "            last_len = layer\n",
    "    \n",
    "        \n",
    "    W_out = np.random.uniform(-1,1,(last_len,num_classes))\n",
    "    W[index] = W_out\n",
    "\n",
    "    return W\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.143236Z",
     "start_time": "2020-04-02T14:26:48.139381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_emb: (5, 10)\n",
      "W_out: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=5,embedding_dim=10,hidden_dim=[], num_classes=2)\n",
    "#vocab_size:d  ,  embedding_dim:\n",
    "print('W_emb:', W[0].shape)\n",
    "print('W_out:', W[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.636732Z",
     "start_time": "2020-04-02T14:26:48.634122Z"
    }
   },
   "outputs": [],
   "source": [
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[2], num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:49.086112Z",
     "start_time": "2020-04-02T14:26:49.082225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_emb: (3, 4)\n",
      "W_h1: (4, 2)\n",
      "W_out: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "print('W_emb:', W[0].shape)\n",
    "print('W_h1:', W[1].shape)\n",
    "print('W_out:', W[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0808575 ,  0.07706537,  0.02544979,  0.04468327],\n",
       "       [-0.09677416,  0.01888638,  0.01135704, -0.06820807],\n",
       "       [-0.0693859 ,  0.03910591, -0.03624671,  0.03839406]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:31:57.970152Z",
     "start_time": "2020-04-01T10:31:57.966123Z"
    }
   },
   "source": [
    "Then you need to develop a `softmax` function (same as in Assignment 1) to be used in the output layer. It takes as input:\n",
    "\n",
    "- `z`: array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the softmax of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:50.504086Z",
     "start_time": "2020-04-02T14:26:50.500686Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    if z.ndim==1:\n",
    "        sig = np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "    else:\n",
    "        sig=[]\n",
    "        for array in z:\n",
    "            temp = np.exp(array)/np.sum(np.exp(array),axis=0)\n",
    "            sig += [temp]\n",
    "        sig = np.array(sig)        \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the categorical cross entropy loss by slightly modifying the function from Assignment 1 to depend only on the true label `y` and the class probabilities vector `y_preds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.360838Z",
     "start_time": "2020-04-02T14:26:51.356935Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_loss(y, y_preds):\n",
    "    \n",
    "    y = np.array(y) - 1 # Because Y_tr and Y_dev start from 1, for indexing, Y should minus 1\n",
    "    if y_preds.ndim==1:\n",
    "        l = -np.log(y_preds)[y]\n",
    "    else:\n",
    "        l_list=[]\n",
    "        for i in range(len(y_preds)):\n",
    "            l_temp = -np.log(y_preds[i])[y[i]]\n",
    "            l_list += [l_temp]\n",
    "        l = sum(l_list)/len(l_list)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.762676Z",
     "start_time": "2020-04-02T14:26:51.758210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_preds:  [0.01217919 0.27035308 0.24462558 0.02710529 0.44573687]\n",
      "loss: 1.3080264848567502\n"
     ]
    }
   ],
   "source": [
    "# example for 5 classes\n",
    "\n",
    "y = 2 #true label\n",
    "y_preds = softmax(np.array([[-2.1,1.,0.9,-1.3,1.5]]))[0]\n",
    "\n",
    "print('y_preds: ',y_preds)\n",
    "print('loss:', categorical_loss(y, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:02:56.149535Z",
     "start_time": "2020-03-31T15:02:56.145738Z"
    }
   },
   "source": [
    "Then, implement the `relu` function to introduce non-linearity after each hidden layer of your network (during the forward pass): \n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "and the `relu_derivative` function to compute its derivative (used in the backward pass):\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{relu_derivative}(z_i)=\\begin{cases}\n",
    "    0, & \\text{if $z_i<=0$}.\\\\\n",
    "    1, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Note that both functions take as input a vector $z$ \n",
    "\n",
    "Hint use .copy() to avoid in place changes in array z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:52.665236Z",
     "start_time": "2020-04-02T14:26:52.661519Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \n",
    "    a = np.maximum(z,0)\n",
    "    \n",
    "    return a\n",
    "    \n",
    "\n",
    "def relu_derivative(z):\n",
    "    \n",
    "    dz=[]\n",
    "    for ele in z:\n",
    "        if ele <= 0:\n",
    "            dz += [0]\n",
    "        else:\n",
    "            dz += [1]\n",
    "    dz=np.array(dz)\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training you should also apply a dropout mask element-wise after the activation function (i.e. vector of ones with a random percentage set to zero). The `dropout_mask` function takes as input:\n",
    "\n",
    "- `size`: the size of the vector that we want to apply dropout\n",
    "- `dropout_rate`: the percentage of elements that will be randomly set to zeros\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `dropout_vec`: a vector with binary values (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.429192Z",
     "start_time": "2020-04-02T14:26:53.425301Z"
    }
   },
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    \n",
    "    dropout_vec = np.ones(size)\n",
    "    zero_num = int(size * dropout_rate)\n",
    "    for i in range(zero_num):\n",
    "        dropout_vec[i] = 0\n",
    "\n",
    "    np.random.shuffle(dropout_vec)\n",
    "    return dropout_vec\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the `forward_pass` function that passes the input x through the network up to the output layer for computing the probability for each class using the weight matrices in `W`. The ReLU activation function should be applied on each hidden layer. \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `dropout_rate`: the dropout rate that is used to generate a random dropout mask vector applied after each hidden layer for regularisation.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `out_vals`: a dictionary of output values from each layer: h (the vector before the activation function), a (the resulting vector after passing h from the activation function), its dropout mask vector; and the prediction vector (probability for each class) from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:54.761268Z",
     "start_time": "2020-04-02T14:26:54.753402Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "    \n",
    "    \n",
    "    out_vals = {}\n",
    "    \n",
    "    h_vecs = []\n",
    "    a_vecs = []\n",
    "    dropout_vecs = []\n",
    "    a_reg_vecs = []\n",
    "    y = 0 # initialise output y    \n",
    "    x_vec_sum=0 # each vector is W[0].shape[1] x 1 ,the sum of embedding vectors is W[0].shape[1] x 1 as well\n",
    "    for word_index in x: # look up the corresponding vectors in embedding matrix for every word index\n",
    "        x_vec_sum += W[0][word_index] # W[0] represents embedding matrix  \n",
    "    h1 = x_vec_sum/len(x) # h1 equals to avreage embedding vectors,according to formula mentioned above\n",
    "    h1 = np.array(h1,dtype='float32')\n",
    "   \n",
    "    h_vecs += [h1]\n",
    "    a1 = relu(h1)\n",
    "    a1 = np.array(a1,dtype='float32')\n",
    "   \n",
    "    a_vecs += [a1]\n",
    "    dropout_vecs1 = dropout_mask(len(a1), dropout_rate)\n",
    "    dropout_vecs += [dropout_vecs1]\n",
    "    a1_regularized = a1 * dropout_vecs1 # use * rather than np.dot\n",
    "    a_reg_vecs += [a1_regularized]\n",
    "    \n",
    "    for i in range(1,len(W)):  # iterate every hidden layer\n",
    "        hi = np.dot(W[i].T,a_reg_vecs[i-1])\n",
    "        hi = np.array(hi,dtype='float32')\n",
    "        if i != (len(W)-1): # if the forwardpass haven't come to the final layer\n",
    "            h_vecs += [hi]\n",
    "            ai = relu(hi)\n",
    "            a_vecs += [ai]\n",
    "            dropout_vec_temp = dropout_mask(len(ai), dropout_rate)\n",
    "            dropout_vecs += [dropout_vec_temp]\n",
    "            ai_regularized = ai * dropout_vec_temp\n",
    "            a_reg_vecs += [ai_regularized]\n",
    "        else: # if it is the final layer, output the y using softmax() rather than relu()\n",
    "            y = softmax(hi)\n",
    "            y = np.array(y,dtype='float32')\n",
    "            \n",
    "    \n",
    "    out_vals['h'] = h_vecs\n",
    "    out_vals['a'] = a_vecs\n",
    "    out_vals['droput_vecs'] = dropout_vecs\n",
    "    out_vals['y'] = y\n",
    "    \n",
    "    return out_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (3, 4)\n",
      "Shape W1 (4, 5)\n",
      "Shape W2 (5, 2)\n",
      "\n",
      "{'h': [array([-0.06532356, -0.0525649 , -0.01648912,  0.03281889], dtype=float32), array([-0.00191901, -0.00136205,  0.00013134,  0.00263806,  0.00317445],\n",
      "      dtype=float32)], 'a': [array([0.        , 0.        , 0.        , 0.03281889], dtype=float32), array([0.        , 0.        , 0.00013134, 0.00263806, 0.00317445],\n",
      "      dtype=float32)], 'droput_vecs': [array([0., 1., 0., 1.]), array([0., 1., 1., 1., 0.])], 'y': array([0.49968764, 0.5003123 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[5], num_classes=2)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "print()\n",
    "print(forward_pass([2,1], W, dropout_rate=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward_pass` function computes the gradients and update the weights for each matrix in the network from the output to the input. It takes as input \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `y`: the true label\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `out_vals`: a dictionary of output values from a forward pass.\n",
    "- `learning_rate`: the learning rate for updating the weights.\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: the updated weights of the network.\n",
    "\n",
    "Hint: the gradients on the output layer are similar to the multiclass logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:56.225630Z",
     "start_time": "2020-04-02T14:26:56.216508Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.001, freeze_emb=False): # this function only used for two layers network\n",
    "    \n",
    "    i = y - 1 # in this case label y start from 1, in order to do indexing, y should minus 1\n",
    "    y_preds = out_vals['y'] # probability y_hat is corresponding to the index of true label y\n",
    "    y_preds[i] = y_preds[i] - 1 #  dL/dH2 = y^ - yi (yi = 1 or 0),the corresponding element should minus 1\n",
    "    h1 = out_vals['h'] #in this case h would only contains one vector h1, because it is only two layers\n",
    "\n",
    "    # the output layer\n",
    "    g = y_preds #  so now we have the vector dl/dh2\n",
    "    a = out_vals['a'][-1]\n",
    "    dW1 = g * a.reshape(len(a),1)\n",
    "    W[1] = W[1] - lr * dW1 # update the weights W[1] which is near the output layer\n",
    "    \n",
    "    \n",
    "    #the first layerï¼Œ\n",
    "    #dL/dW_emb = dL/dH2 x dH2/dA1 * dA1/dH1 * dH1/dW_emb , according to chain rule\n",
    "    if freeze_emb==False:\n",
    "        dW0 = g.dot(W[1].T) * relu_derivative(h1[0]) * (1/len(x)) #dW0 is a vector\n",
    "        for index in x: # update embedding weights accroding to input index  \n",
    "            W[0][index] = W[0][index] - lr*dW0\n",
    "    \n",
    "    return W\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support back-propagation by using the `forward_pass` and `backward_pass` functions.\n",
    "\n",
    "The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `W`: the weights of the network (dictionary)\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `dropout`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated (to be used by the backward pass function).\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.001, \n",
    "        dropout=0.2, epochs=5, tolerance=0.001, freeze_emb=False, print_progress=True):\n",
    "    \n",
    "    cur_loss_tr = 1.\n",
    "    cur_loss_dev = 1.\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    \n",
    "\n",
    "    train_data = list(zip(X_tr,Y_tr))\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        np.random.shuffle(train_data) #randomise training data\n",
    "        yp_train_list = [] # a 2D list of predicted y , each element is a vector after passing softmax \n",
    "        yp_dev_list = []\n",
    "        \n",
    "        for xi,yi in train_data:\n",
    "            train_output = forward_pass(xi, W, dropout)\n",
    "            W = backward_pass(xi, yi, W, train_output,lr,freeze_emb) # W is a global varible\n",
    "            \n",
    "            \n",
    "        for xi,yi in train_data:\n",
    "            train_output = forward_pass(xi, W, dropout)\n",
    "            yp_train_list += [train_output['y']] \n",
    "        \n",
    "        for xi in X_dev:\n",
    "            dev_output = forward_pass(xi, W, dropout)\n",
    "            yp_dev_list += [dev_output['y']] \n",
    "        \n",
    "        Y_tr_random = [train_data[i][1] for i in range(len(train_data))] #randomised true label for tr data\n",
    "        Y_tr_random = np.array(Y_tr_random)\n",
    "        yp_train_list = np.array(yp_train_list) \n",
    "        yp_dev_list = np.array(yp_dev_list) \n",
    "        \n",
    "        loss_tr = categorical_loss(Y_tr_random, yp_train_list)\n",
    "        loss_dev = categorical_loss(Y_dev, yp_dev_list)\n",
    "        training_loss_history += [loss_tr]\n",
    "        validation_loss_history += [loss_dev]\n",
    "        \n",
    "        if abs(cur_loss_dev - loss_dev) <tolerance:\n",
    "            break\n",
    "        if print_progress==True:\n",
    "            print(\"Epoch:\",e,\"| Training loss:\",cur_loss_tr,\"| Validation loss:\",cur_loss_dev)\n",
    "            cur_loss_tr = loss_tr\n",
    "            cur_loss_dev = loss_dev\n",
    "    \n",
    "    return W, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate you neural net. First, you need to define your network using the `network_weights` function followed by SGD with backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:33.643515Z",
     "start_time": "2020-04-02T15:09:33.640943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (8932, 300)\n",
      "Shape W1 (300, 3)\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(id2word),embedding_dim=300,hidden_dim=[], num_classes=3)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:33.643515Z",
     "start_time": "2020-04-02T15:09:33.640943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 1.0 | Validation loss: 1.0\n",
      "Epoch: 1 | Training loss: 1.083648540750146 | Validation loss: 1.0888576058546702\n",
      "Epoch: 2 | Training loss: 1.0642584223300218 | Validation loss: 1.0762688903013866\n",
      "Epoch: 3 | Training loss: 1.0446144733577967 | Validation loss: 1.0704683323701223\n",
      "Epoch: 4 | Training loss: 1.0242512222379445 | Validation loss: 1.062878748178482\n",
      "Epoch: 5 | Training loss: 1.0057844543705383 | Validation loss: 1.046689027150472\n",
      "Epoch: 6 | Training loss: 0.9844742592672507 | Validation loss: 1.042882864077886\n",
      "Epoch: 7 | Training loss: 0.9647397878393531 | Validation loss: 1.0272008415063223\n",
      "Epoch: 8 | Training loss: 0.9454896038770676 | Validation loss: 1.0146472692489623\n",
      "Epoch: 9 | Training loss: 0.9265543546527624 | Validation loss: 1.0034232461452484\n",
      "Epoch: 10 | Training loss: 0.9065539456283053 | Validation loss: 0.9881362215677897\n",
      "Epoch: 11 | Training loss: 0.8856983471910159 | Validation loss: 0.9788456118106842\n",
      "Epoch: 12 | Training loss: 0.8680866953978936 | Validation loss: 0.9663264954090118\n",
      "Epoch: 13 | Training loss: 0.8493476579027871 | Validation loss: 0.9598791062831878\n",
      "Epoch: 14 | Training loss: 0.8298345554247498 | Validation loss: 0.948669251203537\n",
      "Epoch: 15 | Training loss: 0.8110249885916709 | Validation loss: 0.9295575292905172\n",
      "Epoch: 16 | Training loss: 0.7932314101917048 | Validation loss: 0.9141900557279586\n",
      "Epoch: 17 | Training loss: 0.7753243504557759 | Validation loss: 0.9083000655968984\n",
      "Epoch: 18 | Training loss: 0.7585326449076335 | Validation loss: 0.8950486322244008\n",
      "Epoch: 19 | Training loss: 0.7417521543583522 | Validation loss: 0.8854310230414073\n",
      "Epoch: 20 | Training loss: 0.7241215203609318 | Validation loss: 0.8752058780193329\n",
      "Epoch: 21 | Training loss: 0.7089350396813825 | Validation loss: 0.8572026032209397\n",
      "Epoch: 22 | Training loss: 0.6934315007536983 | Validation loss: 0.848803573846817\n",
      "Epoch: 23 | Training loss: 0.6780105121992528 | Validation loss: 0.8403338404496511\n",
      "Epoch: 24 | Training loss: 0.6646641232368226 | Validation loss: 0.8213913255929947\n",
      "Epoch: 25 | Training loss: 0.6502455526031554 | Validation loss: 0.8185259992877643\n",
      "Epoch: 26 | Training loss: 0.6367154316355784 | Validation loss: 0.8012540704011917\n",
      "Epoch: 27 | Training loss: 0.6245894996371741 | Validation loss: 0.7863441550731659\n",
      "Epoch: 28 | Training loss: 0.6103005116308728 | Validation loss: 0.7791214702526729\n",
      "Epoch: 29 | Training loss: 0.5980884270020761 | Validation loss: 0.7752959022919337\n",
      "Epoch: 30 | Training loss: 0.585775064988217 | Validation loss: 0.7551130990187327\n",
      "Epoch: 31 | Training loss: 0.5736653086155032 | Validation loss: 0.7513792264461517\n",
      "Epoch: 32 | Training loss: 0.5616447399999015 | Validation loss: 0.7375143947203954\n",
      "Epoch: 33 | Training loss: 0.5508644689798045 | Validation loss: 0.7269196155667305\n",
      "Epoch: 34 | Training loss: 0.539257463606773 | Validation loss: 0.717248031993707\n",
      "Epoch: 35 | Training loss: 0.5314568905012371 | Validation loss: 0.7087907079855601\n",
      "Epoch: 36 | Training loss: 0.5202074376117283 | Validation loss: 0.7044272429744403\n",
      "Epoch: 37 | Training loss: 0.5102362386928871 | Validation loss: 0.6916620639463266\n",
      "Epoch: 38 | Training loss: 0.49979107865171196 | Validation loss: 0.6871511391798655\n",
      "Epoch: 39 | Training loss: 0.4904523257612406 | Validation loss: 0.6720315522452196\n",
      "Epoch: 40 | Training loss: 0.48345845376413005 | Validation loss: 0.6677986899514993\n",
      "Epoch: 41 | Training loss: 0.474815222780259 | Validation loss: 0.6581074634194374\n",
      "Epoch: 42 | Training loss: 0.46747308075039956 | Validation loss: 0.6574995532631874\n",
      "Epoch: 43 | Training loss: 0.4594286134565482 | Validation loss: 0.6482041323184967\n",
      "Epoch: 44 | Training loss: 0.4489836418083481 | Validation loss: 0.6339804790914059\n",
      "Epoch: 45 | Training loss: 0.4419403319006475 | Validation loss: 0.6319276382029057\n",
      "Epoch: 46 | Training loss: 0.43556484203397605 | Validation loss: 0.6165127312143643\n",
      "Epoch: 47 | Training loss: 0.4267556197250572 | Validation loss: 0.617348380535841\n",
      "Epoch: 48 | Training loss: 0.4204419378740325 | Validation loss: 0.6123680869241556\n",
      "Epoch: 49 | Training loss: 0.4157299045009616 | Validation loss: 0.6066032356023788\n",
      "Epoch: 50 | Training loss: 0.40573469278092184 | Validation loss: 0.6025536640733481\n",
      "Epoch: 51 | Training loss: 0.40082941222470253 | Validation loss: 0.5937911537537972\n",
      "Epoch: 52 | Training loss: 0.3930902272193149 | Validation loss: 0.591344712699453\n",
      "Epoch: 53 | Training loss: 0.38871122378623113 | Validation loss: 0.5865696377803882\n",
      "Epoch: 54 | Training loss: 0.3827702323095097 | Validation loss: 0.5816100918253263\n",
      "Epoch: 55 | Training loss: 0.3758980107029978 | Validation loss: 0.5715380706141392\n",
      "Epoch: 56 | Training loss: 0.37368718213411434 | Validation loss: 0.5691479200621447\n",
      "Epoch: 57 | Training loss: 0.36515904914733255 | Validation loss: 0.5612069941063722\n",
      "Epoch: 58 | Training loss: 0.3592827203440053 | Validation loss: 0.5680446368455887\n",
      "Epoch: 59 | Training loss: 0.3554439155284005 | Validation loss: 0.5421678685148557\n",
      "Epoch: 60 | Training loss: 0.3512911372450374 | Validation loss: 0.5455399554222822\n",
      "Epoch: 61 | Training loss: 0.345148814508672 | Validation loss: 0.5445189883808295\n",
      "Epoch: 62 | Training loss: 0.34124602485525735 | Validation loss: 0.5438891926159461\n",
      "Epoch: 63 | Training loss: 0.3367892312402061 | Validation loss: 0.5303035869821906\n",
      "Epoch: 64 | Training loss: 0.3329867292970691 | Validation loss: 0.5244553203011553\n",
      "Epoch: 65 | Training loss: 0.32673840276664123 | Validation loss: 0.514719059827427\n",
      "Epoch: 66 | Training loss: 0.32074573943881357 | Validation loss: 0.532265639975667\n",
      "Epoch: 67 | Training loss: 0.3185207171017343 | Validation loss: 0.5296326616654793\n",
      "Epoch: 68 | Training loss: 0.3125211553290137 | Validation loss: 0.5115733042607705\n",
      "Epoch: 69 | Training loss: 0.31127253060510457 | Validation loss: 0.5145761088654399\n",
      "Epoch: 70 | Training loss: 0.3046941290831698 | Validation loss: 0.501851190601786\n",
      "Epoch: 71 | Training loss: 0.3019389199730358 | Validation loss: 0.5026578628395995\n",
      "Epoch: 72 | Training loss: 0.2966291260183789 | Validation loss: 0.5043858020255964\n",
      "Epoch: 73 | Training loss: 0.29349187982899216 | Validation loss: 0.500357065051794\n",
      "Epoch: 74 | Training loss: 0.29129302677485003 | Validation loss: 0.4889841712266207\n",
      "Epoch: 75 | Training loss: 0.2869298341666581 | Validation loss: 0.4896386756002903\n",
      "Epoch: 76 | Training loss: 0.2819181558199974 | Validation loss: 0.4903209431717793\n",
      "Epoch: 77 | Training loss: 0.2797542291151573 | Validation loss: 0.4744357647312184\n",
      "Epoch: 78 | Training loss: 0.27821352878536953 | Validation loss: 0.47268368302534025\n",
      "Epoch: 79 | Training loss: 0.2728570042318703 | Validation loss: 0.48742004901791613\n",
      "Epoch: 80 | Training loss: 0.26969148160424083 | Validation loss: 0.4765715487611791\n",
      "Epoch: 81 | Training loss: 0.2662730509416239 | Validation loss: 0.4698698482910792\n",
      "Epoch: 82 | Training loss: 0.2639651802857649 | Validation loss: 0.46252525215347606\n",
      "Epoch: 83 | Training loss: 0.258538563354911 | Validation loss: 0.4645335408238073\n",
      "Epoch: 84 | Training loss: 0.2583015019518401 | Validation loss: 0.4632942846417427\n",
      "Epoch: 85 | Training loss: 0.2553279156967377 | Validation loss: 0.46014172459021213\n",
      "Epoch: 86 | Training loss: 0.25122887483715506 | Validation loss: 0.451740106344223\n",
      "Epoch: 87 | Training loss: 0.24846872638937686 | Validation loss: 0.45382053731630245\n",
      "Epoch: 88 | Training loss: 0.24622846618653663 | Validation loss: 0.45163492221385243\n",
      "Epoch: 89 | Training loss: 0.2423242203920381 | Validation loss: 0.45086705924322207\n",
      "Epoch: 90 | Training loss: 0.2407357838213405 | Validation loss: 0.4380331508970509\n"
     ]
    }
   ],
   "source": [
    "W, loss_tr, dev_loss = SGD(X_tr, Y_tr,\n",
    "                            W,\n",
    "                            X_dev=X_dev, \n",
    "                            Y_dev=Y_dev,\n",
    "                            lr=0.002, \n",
    "                            dropout=0.2,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.0001,\n",
    "                            epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1db4c793400>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=range(0,len(loss_tr))\n",
    "y1=loss_tr\n",
    "y2=dev_loss\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Monitoring')\n",
    "plt.plot(x, y1,'-',label=\"train loss\")\n",
    "plt.plot(x, y2,'-',label=\"valid loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:10:11.037495Z",
     "start_time": "2020-04-02T15:10:11.034999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8488888888888889\n",
      "Precision: 0.8551595134336046\n",
      "Recall: 0.8488888888888889\n",
      "F1-Score: 0.8492939160421709\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y'])+1 for x,y in zip(X_te,Y_te)]\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Tuning Records for The Model ###\n",
    "\n",
    "\n",
    "| Parameters | Precision  | Recall  | F1-Score  | Accuracy |  Comment |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|lr=0.01,dropout=0.2,emb_dim=300   |0.8591   |0.8522   |0.8531   |0.8522 | **a relatively good results**|\n",
    "|lr=0.01,dropout=0.5,emb_dim=300   | 0.3433 |0.3414   |0.3213   |0.3324 | didn't learn, increase lr|\n",
    "| lr=0.1,dropout=0.5,emb_dim=300   | 0.8541 |0.8533   |0.8534   |0.8533 | **a relatively good results**|\n",
    "|lr=0.01,dropout=0.2,emb_dim=50  | 0.3759  |  0.3744 | 0.3204  |0.3744| didn't learn, increase lr||\n",
    "|lr=0.1,dropout=0.2,emb_dim=50  | 0.8399  |  0.8356 | 0.8353  |0.8356|  Although result is relatively good, but loss began to increase at last few epochs, need to try increasing dropout|\n",
    "|lr=0.1,dropout=0.5,emb_dim=50  |  ||||training was stopped, loss diverged|\n",
    "|lr=0.1,dropout=0.2,emb_dim=500  |0.8533   | 0.8511  | 0.8511  |0.8511| **relatively good result, both train_loss and dev_loss decreased to a very value with few steps**|\n",
    "|lr=0.1,dropout=0.5,emb_dim=500  |0.8577   | 0.8533  | 0.8511  |0.8511|  **relatively good result, both train_loss and dev_loss decreased to a very value with few steps**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, to see if the loss trends are correctly decreasing, I didn't set tolerance. When I saw the validation loss and training loss kept decreasing, I can sure the SGD function are basicly correct. The next step is to choose model hyperparameters, I printed the validation loss and looked through the loss value, so that I can choose a proper value of tolerance to make sure the loss converge so effectively. The parameter tuning records table is shown above. Sometimes when learning rate is to small it doesn't learn very quickly, validation loss may be judged as converged, so when it doesn't learn, I would try to increase learning rate or use smaller tolerance. Basically, for the first model, it can achieve F1-score around 0.83--0.85. The bigger embedding size, the less epochs, but it would take very long time to compute a single epoch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained Embeddings\n",
    "\n",
    "Now re-train the network using GloVe pre-trained embeddings. You need to modify the `backward_pass` function above to stop computing gradients and updating weights of the embedding matrix.\n",
    "\n",
    "Use the function below to obtain the embedding martix for your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:32.020697Z",
     "start_time": "2020-04-02T14:27:32.015733Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, word2id, emb_size=300):\n",
    "    \n",
    "    w_emb = np.zeros((len(word2id), emb_size))\n",
    "    \n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split()[0]\n",
    "                     \n",
    "                if word in vocab:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[word2id[word]] += emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8932, 300)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_glove = get_glove_embeddings(\"glove.840B.300d.zip\",\"glove.840B.300d.txt\",word2id)\n",
    "\n",
    "w_glove.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the weights of your network using the `network_weights` function. Second, replace the weigths of the embedding matrix with `w_glove`. Finally, train the network by freezing the embedding weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (8932, 300)\n",
      "Shape W1 (300, 3)\n"
     ]
    }
   ],
   "source": [
    "W_glove = network_weights(vocab_size=len(word2id),embedding_dim=300,hidden_dim=[], num_classes=3)#initialise the weights\n",
    "\n",
    "for i in range(len(W_glove)):\n",
    "    print('Shape W'+str(i), W_glove[i].shape)\n",
    "\n",
    "W_glove[0] = w_glove # replace the weigths of the embedding matrix with w_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:30:11.121198Z",
     "start_time": "2020-04-02T14:29:24.946124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 1.0 | Validation loss: 1.0\n",
      "Epoch: 1 | Training loss: 0.9136370339120428 | Validation loss: 0.906350563466549\n",
      "Epoch: 2 | Training loss: 0.8560330870809655 | Validation loss: 0.8568824462095896\n",
      "Epoch: 3 | Training loss: 0.8054317477873216 | Validation loss: 0.8503042661150296\n",
      "Epoch: 4 | Training loss: 0.759856741965438 | Validation loss: 0.7820344890157381\n",
      "Epoch: 5 | Training loss: 0.7237736781112228 | Validation loss: 0.7677977581818899\n",
      "Epoch: 6 | Training loss: 0.6991021084195623 | Validation loss: 0.7337598125636577\n",
      "Epoch: 7 | Training loss: 0.6770654456476526 | Validation loss: 0.6953079197307428\n",
      "Epoch: 8 | Training loss: 0.647945334391358 | Validation loss: 0.6593279253443082\n",
      "Epoch: 9 | Training loss: 0.6143279932090081 | Validation loss: 0.6643118913968404\n",
      "Epoch: 10 | Training loss: 0.6096981692151167 | Validation loss: 0.6293257150053978\n",
      "Epoch: 11 | Training loss: 0.5890490328357555 | Validation loss: 0.6583368104199568\n",
      "Epoch: 12 | Training loss: 0.5813919939496555 | Validation loss: 0.6178573520978292\n",
      "Epoch: 13 | Training loss: 0.5743712862678028 | Validation loss: 0.5964599508792162\n",
      "Epoch: 14 | Training loss: 0.5498703877814114 | Validation loss: 0.5757141739378373\n",
      "Epoch: 15 | Training loss: 0.5447533513442613 | Validation loss: 0.5967822500566642\n",
      "Epoch: 16 | Training loss: 0.5434551506272207 | Validation loss: 0.568485323258986\n",
      "Epoch: 17 | Training loss: 0.5221981858718209 | Validation loss: 0.5508582007388274\n",
      "Epoch: 18 | Training loss: 0.5221150669211057 | Validation loss: 0.5661832394450903\n",
      "Epoch: 19 | Training loss: 0.5203578431931479 | Validation loss: 0.5243460288147131\n",
      "Epoch: 20 | Training loss: 0.5082030924481418 | Validation loss: 0.5692009396975239\n",
      "Epoch: 21 | Training loss: 0.5155480321931343 | Validation loss: 0.5227212885643046\n",
      "Epoch: 22 | Training loss: 0.5003222109585962 | Validation loss: 0.5150238406658173\n",
      "Epoch: 23 | Training loss: 0.493950724688669 | Validation loss: 0.5171962768584489\n",
      "Epoch: 24 | Training loss: 0.5000595136194897 | Validation loss: 0.5050061728867392\n",
      "Epoch: 25 | Training loss: 0.4960635637495822 | Validation loss: 0.4756477637899419\n",
      "Epoch: 26 | Training loss: 0.4965720971822157 | Validation loss: 0.5194769988084833\n",
      "Epoch: 27 | Training loss: 0.487651781841317 | Validation loss: 0.5410987181340655\n",
      "Epoch: 28 | Training loss: 0.4835824313206831 | Validation loss: 0.5063689333138367\n",
      "Epoch: 29 | Training loss: 0.47583955496326474 | Validation loss: 0.45896475882579885\n",
      "Epoch: 30 | Training loss: 0.4687729954020081 | Validation loss: 0.48481883520881336\n",
      "Epoch: 31 | Training loss: 0.47831836147767415 | Validation loss: 0.4745459225277106\n",
      "Epoch: 32 | Training loss: 0.4806333314558045 | Validation loss: 0.44712893772870305\n",
      "Epoch: 33 | Training loss: 0.4534148421230687 | Validation loss: 0.4668026242715617\n",
      "Epoch: 34 | Training loss: 0.4510433634412281 | Validation loss: 0.491865271780019\n",
      "Epoch: 35 | Training loss: 0.45847450792052163 | Validation loss: 0.5153895422288527\n",
      "Epoch: 36 | Training loss: 0.46500710262795714 | Validation loss: 0.47260852992534635\n",
      "Epoch: 37 | Training loss: 0.45184111976161756 | Validation loss: 0.4382781430892646\n",
      "Epoch: 38 | Training loss: 0.4471367042259468 | Validation loss: 0.4565529870055616\n",
      "Epoch: 39 | Training loss: 0.45225187347282675 | Validation loss: 0.44725530673439307\n",
      "Epoch: 40 | Training loss: 0.4481110795060037 | Validation loss: 0.4666171924304217\n",
      "Epoch: 41 | Training loss: 0.44023397773836526 | Validation loss: 0.432455408644552\n",
      "Epoch: 42 | Training loss: 0.45089695368306515 | Validation loss: 0.42641285471928614\n",
      "Epoch: 43 | Training loss: 0.4308569405883221 | Validation loss: 0.47102757196562983\n",
      "Epoch: 44 | Training loss: 0.435840696042942 | Validation loss: 0.45070467789657415\n",
      "Epoch: 45 | Training loss: 0.444427076274175 | Validation loss: 0.4303118441874782\n",
      "Epoch: 46 | Training loss: 0.44862051720211943 | Validation loss: 0.4471811775987347\n"
     ]
    }
   ],
   "source": [
    "W_glove, loss_tr_glove, dev_loss_glove = SGD(X_tr, Y_tr,\n",
    "                            W_glove,\n",
    "                            X_dev=X_dev, \n",
    "                            Y_dev=Y_dev,\n",
    "                            lr=0.001, \n",
    "                            dropout=0.1,\n",
    "                            freeze_emb=True, # stop updating W[0]\n",
    "                            tolerance=0.001,\n",
    "                            epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1db4ddba748>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=range(0,len(loss_tr_glove))\n",
    "y1=loss_tr_glove\n",
    "y2=dev_loss_glove\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Monitoring')\n",
    "plt.plot(x, y1,'-',label=\"train loss\")\n",
    "plt.plot(x, y2,'-',label=\"valid loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:12:00.815184Z",
     "start_time": "2020-04-02T15:12:00.812563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8655555555555555\n",
      "Precision: 0.8676440621565756\n",
      "Recall: 0.8655555555555555\n",
      "F1-Score: 0.8660015472520705\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W_glove, dropout_rate=0.0)['y'])+1 for x,y in zip(X_te,Y_te)]\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Tuning Records for The Model \n",
    "\n",
    "| Parameters | Precision  | Recall  | F1-Score  | Accuracy |  Comment |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    " | lr=0.001,dropout=0.2,emb_dim=300   |   |   |   | |stopped training, In this case I try to make tolerance 0.001, but loss kept vibrating|\n",
    " | lr=0.0001,dropout=0.2,emb_dim=300   | 0.8742 |0.8733   |0.8736   |0.8733 | **a relatively good results**|\n",
    " | lr=0.0001,dropout=0.5,emb_dim=300   | 0.8801 |0.8800   |0.8794   |0.88 | **Results were good, but loss kept vibrating**|\n",
    " | lr=0.001,dropout=0.5,emb_dim=300   |  |   |   | |stopped training, loss diverge|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I used the Glove with the embedding_size=300. At first, I tried learning_rate=0.001,dropout=0.2, default tolerance is 0.001, but loss values kept vabrating, so I used a smaller learning rate, 0.0001, and the loss started to decrease normally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to support deeper architectures (Bonus)\n",
    "\n",
    "Extend the network to support back-propagation for more hidden layers. You need to modify the `backward_pass` function above to compute gradients and update the weights between intermediate hidden layers. Finally, train and evaluate a network with a deeper architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:58:51.764619Z",
     "start_time": "2020-04-02T14:58:47.483690Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.001, freeze_emb=False): # this backward function used for bonus\n",
    "    \n",
    "    i = y - 1 # in this case label y start from 1, in order to do indexing, y should minus 1\n",
    "    l = len(W) - 1 # the max index for W\n",
    "    y_preds = out_vals['y'] # probability y_hat is corresponding to the index of true label y\n",
    "    y_preds[i] = y_preds[i] - 1 #  dL/dHl = y^ - yi (yi = 1 or 0),the corresponding element should minus 1\n",
    "    h = out_vals['h'] \n",
    "    \n",
    "    # update the output layer weight\n",
    "    g = y_preds #  so now we have the vector dl/dHl\n",
    "    a = out_vals['a']\n",
    "    dWl =  g * a[-1].reshape(len(a[-1]),1) # the last layer before output layer\n",
    "    W[l] = W[l] - lr * dWl # update the weights W[1] which is near the output layer\n",
    "    dA = W[l].dot(g)#this is dL/dA[l-1]\n",
    "    \n",
    "    # update the hidden layers' weights\n",
    "    for i in range(l-1,0,-1): # from W[l-1] to W[1], because W[0] is pre-trained embedding matrix and don't need to be updated\n",
    "        dH = relu_derivative(h[i]) * dA # layer l - 1\n",
    "        dWi =  dH * a[i-1].reshape(len(a[i-1]),1) # compute gradients, dWi should have same dimension with W[i]\n",
    "        W[i] = W[i] - lr * dWi\n",
    "        dA = W[i].dot(dH)\n",
    "   \n",
    "    \n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (8932, 300)\n",
      "Shape W1 (300, 18)\n",
      "Shape W2 (18, 6)\n",
      "Shape W3 (6, 3)\n"
     ]
    }
   ],
   "source": [
    "W_extend = network_weights(vocab_size=len(word2id),embedding_dim=300,hidden_dim=[18,6], num_classes=3)#initialise the weights with 2 hidden layers\n",
    "\n",
    "for i in range(len(W_extend)):\n",
    "    print('Shape W'+str(i), W_extend[i].shape)\n",
    "\n",
    "W_extend[0] = w_glove # replace the weigths of the embedding matrix with w_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 1.0 | Validation loss: 1.0\n",
      "Epoch: 1 | Training loss: 1.0913581935564678 | Validation loss: 1.0937944134076436\n",
      "Epoch: 2 | Training loss: 1.0778123780091604 | Validation loss: 1.0839935421943665\n",
      "Epoch: 3 | Training loss: 1.0511916650086641 | Validation loss: 1.0620849593480428\n",
      "Epoch: 4 | Training loss: 0.998961207618316 | Validation loss: 1.010606731971105\n",
      "Epoch: 5 | Training loss: 0.9198904952158531 | Validation loss: 0.9378365512688954\n",
      "Epoch: 6 | Training loss: 0.8264042943902313 | Validation loss: 0.8198444966475169\n",
      "Epoch: 7 | Training loss: 0.7445641990533719 | Validation loss: 0.7659697573383649\n",
      "Epoch: 8 | Training loss: 0.6811176009150222 | Validation loss: 0.6939159691830477\n",
      "Epoch: 9 | Training loss: 0.6267736328051736 | Validation loss: 0.5935088524222374\n",
      "Epoch: 10 | Training loss: 0.5781008933034415 | Validation loss: 0.5965038308997949\n",
      "Epoch: 11 | Training loss: 0.5241973004955799 | Validation loss: 0.49324225418269635\n",
      "Epoch: 12 | Training loss: 0.5081594154161091 | Validation loss: 0.4772186750297745\n"
     ]
    }
   ],
   "source": [
    "W_extend, loss_tr_extend, dev_loss_extend = SGD(X_tr, Y_tr,\n",
    "                            W_extend,\n",
    "                            X_dev=X_dev, \n",
    "                            Y_dev=Y_dev,\n",
    "                            lr=0.001, \n",
    "                            dropout=0.1,\n",
    "                            freeze_emb=True, # stop updating W[0]\n",
    "                            tolerance=0.0005,\n",
    "                            epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1db4dcf9fd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=range(0,len(loss_tr_extend))\n",
    "y1=loss_tr_extend\n",
    "y2=dev_loss_extend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Monitoring')\n",
    "plt.plot(x, y1,'-',label=\"train loss\")\n",
    "plt.plot(x, y2,'-',label=\"valid loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:11:51.994986Z",
     "start_time": "2020-04-02T15:11:51.992563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8833333333333333\n",
      "Precision: 0.8829641419531535\n",
      "Recall: 0.8833333333333334\n",
      "F1-Score: 0.8830386824903247\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W_extend, dropout_rate=0.0)['y'])+1 for x,y in zip(X_te,Y_te)]\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "Add your final results here:\n",
    "\n",
    "| Model | Precision  | Recall  | F1-Score  | Accuracy\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| Average Embedding  | 0.855  | 0.849  | 0.849  | 0.849  |\n",
    "| Average Embedding (Pre-trained)  | 0.868  |0.866   |  0.866 |0.866   |\n",
    "| Average Embedding (Pre-trained) + 2 hidden layers (BONUS)   | 0.883  | 0.883  | 0.883  | 0.883  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, test results are:\n",
    "Average embedding, the F1-Score around 0.84 - 0.85; \n",
    "Average embedding(Pre-trained), the F1-Score around 0.86 - 0.88; \n",
    "Average embedding(Pre-trained), the F1-Score around 0.87 - 0.89; \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
